# streamlit_app.py
# ì‹¤í–‰: streamlit run streamlit_app.py
# í•„ìš” íŒ¨í‚¤ì§€: streamlit, pandas, numpy, pydeck, openpyxl

import streamlit as st
import pandas as pd
import numpy as np
import pydeck as pdk
from datetime import datetime, timedelta, time as dtime
from io import BytesIO
from zoneinfo import ZoneInfo
import os, re

KST = ZoneInfo("Asia/Seoul")

# =============================
# ì „ì—­ UI (í° ê¸€ì”¨/í° ë²„íŠ¼)
# =============================
st.set_page_config(page_title="ğŸ§¡ ë…ê±°ë…¸ì¸ ì§€ì› ì›¹ì•± (Prototype)", page_icon="ğŸ§¡", layout="wide")
st.markdown("""
<style>
:root { --base-font: 20px; }
html, body, [class*="css"]  { font-size: var(--base-font); }
button, .stButton>button { font-size: 1.1rem !important; padding: 0.6rem 1.1rem !important; border-radius: 12px !important; }
input, select, textarea, .stTextInput>div>div>input { font-size: 1.05rem !important; }
thead tr th { font-size: 1.05rem !important; }
</style>
""", unsafe_allow_html=True)

st.title("ğŸ§¡ ë…ê±°ë…¸ì¸ ì§€ì› ì›¹ì•± (Prototype)")

# =============================
# íŒŒì¼ ê²½ë¡œ / ìƒìˆ˜
# =============================
CHECKIN_CSV = "checkins.csv"
MEDS_CSV = "meds.csv"
MEDLOG_CSV = "med_log.csv"
INSTITUTIONS_CSV = "institutions.csv"      # ë‚´ë¶€ í‘œì¤€ ìºì‹œ
REGIONAL_CSV = "regional_factors.csv"      # ë‚´ë¶€ í‘œì¤€ ìºì‹œ
HOME_JSON = "home_location.json"           # ì§‘ ìœ„ì¹˜ ì €ì¥

# ì‚¬ìš©ìê°€ ë¯¸ë¦¬ ì˜¬ë¦° ì›ë³¸ íŒŒì¼ ê²½ë¡œ(ìˆìœ¼ë©´ ìë™ ë°˜ì˜)
USER_INST_CANDIDATES = [
    "/mnt/data/ì „êµ­ì˜ë£Œê¸°ê´€ í‘œì¤€ë°ì´í„°.csv",
    "ì „êµ­ì˜ë£Œê¸°ê´€ í‘œì¤€ë°ì´í„°.csv"
]
USER_REG_CANDIDATES = [
    "/mnt/data/ë…ê±°ë…¸ì¸ê°€êµ¬ë¹„ìœ¨_ì‹œë„_ì‹œ_êµ°_êµ¬__20251029204458.xlsx",
    "ë…ê±°ë…¸ì¸ê°€êµ¬ë¹„ìœ¨_ì‹œë„_ì‹œ_êµ°_êµ¬__20251029204458.xlsx"
]

# =============================
# ìœ í‹¸
# =============================
def now_kst():
    return datetime.now(KST)

def load_csv(path, dtype=None, parse_dates=None):
    if os.path.exists(path):
        try:
            return pd.read_csv(path, dtype=dtype, parse_dates=parse_dates)
        except Exception:
            return pd.DataFrame()
    return pd.DataFrame()

def save_csv(df, path):
    if isinstance(df, pd.DataFrame):
        df.to_csv(path, index=False)

def try_read_first_exists(paths):
    for p in paths:
        if os.path.exists(p):
            return p
    return None

def make_alarm_wav(seconds=2, freq=880, sr=16000):
    import wave, struct
    t = np.linspace(0, seconds, int(sr*seconds), False)
    tone = (0.5*np.sin(2*np.pi*freq*t)).astype(np.float32)
    buf = BytesIO()
    with wave.open(buf, 'wb') as w:
        w.setnchannels(1); w.setsampwidth(2); w.setframerate(sr)
        for s in tone:
            w.writeframes(struct.pack('<h', int(s*32767)))
    buf.seek(0)
    return buf

ALARM_WAV = make_alarm_wav()

def parse_time_str(tstr):
    try:
        h, m = map(int, str(tstr).split(":"))
        return dtime(hour=h, minute=m)
    except Exception:
        return None

# --- ì§‘ ìœ„ì¹˜ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸° ---
def load_home():
    import json
    if os.path.exists(HOME_JSON):
        try:
            with open(HOME_JSON, "r", encoding="utf-8") as f:
                return json.load(f)  # {"label": str, "lat": float, "lon": float}
        except Exception:
            return None
    return None

def save_home(lat: float, lon: float, label: str = "ìš°ë¦¬ ì§‘"):
    import json
    data = {"label": label, "lat": float(lat), "lon": float(lon)}
    with open(HOME_JSON, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False)
    return data

def delete_home():
    try:
        os.remove(HOME_JSON)
    except FileNotFoundError:
        pass

# =============================
# ì‚¬ìš©ì ì›ë³¸ -> ë‚´ë¶€ í‘œì¤€ ë³€í™˜
# institutions: name,type,lat,lon,address,region_name
# regional: region_name, solo_ratio(0~1), accessibility_score(optional)
# =============================
def normalize_institutions(df: pd.DataFrame) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=["name","type","lat","lon","address","region_name"])
    d = df.copy()
    colmap = {}
    for c in d.columns:
        lc = c.lower()
        if lc in ["ìš”ì–‘ê¸°ê´€ëª…","ê¸°ê´€ëª…","name","inst_name","ëª…ì¹­"]: colmap[c] = "name"
        elif lc in ["ì¢…ë³„ì½”ë“œëª…","ì¢…ë³„ì½”ë“œ","ì¢…ë³„","ìœ í˜•","type","category"]: colmap[c] = "type"
        elif lc in ["ìœ„ë„","lat","latitude","y","ì¢Œí‘œy","ì¢Œí‘œ_y"]: colmap[c] = "lat"
        elif lc in ["ê²½ë„","lon","lng","longitude","x","ì¢Œí‘œx","ì¢Œí‘œ_x"]: colmap[c] = "lon"
        elif any(k in lc for k in ["ë„ë¡œëª…ì£¼ì†Œ","ì§€ë²ˆì£¼ì†Œ","ì£¼ì†Œ","address"]): colmap[c] = "address"
        elif lc in ["ì‹œë„ëª…","ì‹œë„","ê´‘ì—­ì‹œë„","ì‹œë„ì½”ë“œëª…"]: colmap[c] = "sido"
        elif lc in ["ì‹œêµ°êµ¬ëª…","ì‹œêµ°êµ¬","ì‹œêµ°êµ¬ì½”ë“œëª…"]: colmap[c] = "sigungu"
    if colmap:
        d = d.rename(columns=colmap)

    for c in ["lat","lon"]:
        if c in d.columns:
            d[c] = pd.to_numeric(d[c], errors="coerce")

    if "type" not in d.columns: d["type"] = ""
    if "address" not in d.columns:
        if "sido" in d.columns or "sigungu" in d.columns:
            d["address"] = (d.get("sido","").astype(str).fillna("") + " " + d.get("sigungu","").astype(str).fillna("")).str.strip()
        else:
            d["address"] = ""

    if "sido" in d.columns:
        d["region_name"] = d["sido"].astype(str)
    else:
        def guess_sido(addr: str):
            if not isinstance(addr, str): return ""
            m = re.match(r"^(ì„œìš¸|ë¶€ì‚°|ëŒ€êµ¬|ì¸ì²œ|ê´‘ì£¼|ëŒ€ì „|ìš¸ì‚°|ì„¸ì¢…|ê²½ê¸°|ê°•ì›|ì¶©ë¶|ì¶©ë‚¨|ì „ë¶|ì „ë‚¨|ê²½ë¶|ê²½ë‚¨|ì œì£¼)", addr)
            return m.group(0) if m else ""
        d["region_name"] = d["address"].astype(str).apply(guess_sido)

    def norm_type(t):
        t = str(t)
        if "ì•½êµ­" in t: return "ì•½êµ­"
        if any(k in t for k in ["ë³‘ì›","ì˜ì›","í•œì˜ì›","ì¹˜ê³¼"]): return "ë³‘ì›"
        return t if t else "ê¸°íƒ€"
    d["type"] = d["type"].apply(norm_type)

    d = d[pd.notna(d["lat"]) & pd.notna(d["lon"])]
    return d[["name","type","lat","lon","address","region_name"]].reset_index(drop=True)

def load_user_institutions():
    p = try_read_first_exists(USER_INST_CANDIDATES)
    if p:
        try:
            raw = pd.read_csv(p)
            return normalize_institutions(raw)
        except Exception as e:
            st.warning(f"ì „êµ­ì˜ë£Œê¸°ê´€ CSV ì½ê¸° ì˜¤ë¥˜: {e}")
    return pd.DataFrame(columns=["name","type","lat","lon","address","region_name"])

def normalize_regional(df_or_path) -> pd.DataFrame:
    if df_or_path is None:
        return pd.DataFrame(columns=["region_name","solo_ratio","accessibility_score"])
    try:
        if isinstance(df_or_path, str):
            if df_or_path.lower().endswith(".xlsx"):
                d = pd.read_excel(df_or_path, engine="openpyxl")
            else:
                d = pd.read_csv(df_or_path)
        else:
            d = df_or_path.copy()
    except Exception as e:
        st.warning(f"ì§€ì—­ ì—‘ì…€/CSV ì½ê¸° ì˜¤ë¥˜: {e}")
        return pd.DataFrame(columns=["region_name","solo_ratio","accessibility_score"])

    if d is None or d.empty:
        return pd.DataFrame(columns=["region_name","solo_ratio","accessibility_score"])

    cols = [c for c in d.columns]
    region_col = None
    for c in cols:
        lc = str(c).lower()
        if any(k in lc for k in ["ì‹œêµ°êµ¬","ì‹œÂ·êµ°Â·êµ¬","ì‹œêµ°","ì‹œë„","ê´‘ì—­","í–‰ì •êµ¬ì—­","ì§€ì—­","ì§€ì—­ëª…"]):
            region_col = c; break
    if region_col is None:
        if d.dtypes.iloc[0] == object: region_col = d.columns[0]
        else:
            d["region_name"] = "ì•Œìˆ˜ì—†ìŒ"; region_col = "region_name"

    numeric_cols = [c for c in cols if pd.api.types.is_numeric_dtype(d[c])]
    value_col = numeric_cols[0] if numeric_cols else None

    out = pd.DataFrame()
    out["region_name"] = d[region_col].astype(str)
    if value_col is not None:
        vals = d[value_col].astype(float)
        out["solo_ratio"] = vals/100.0 if vals.max() > 1.5 else vals
    else:
        out["solo_ratio"] = 0.0
    out = out.groupby("region_name", as_index=False)["solo_ratio"].mean()
    out["accessibility_score"] = np.nan
    return out

def load_user_regional():
    p = try_read_first_exists(USER_REG_CANDIDATES)
    if p: return normalize_regional(p)
    return pd.DataFrame(columns=["region_name","solo_ratio","accessibility_score"])

# =============================
# ì²´í¬ì¸/ë³µì•½/ìœ„í—˜ë„
# =============================
def checkin_stats(df: pd.DataFrame, lookback_days=30):
    if df.empty:
        return {"missing_days": [], "z_outliers_idx": [], "mean_min": None, "std_min": None}
    df_recent = df[df["timestamp"] >= (now_kst() - timedelta(days=lookback_days))]
    if df_recent.empty:
        return {"missing_days": [], "z_outliers_idx": [], "mean_min": None, "std_min": None}
    daily = (df_recent
             .assign(date=lambda x: x["timestamp"].dt.date,
                     minutes=lambda x: x["timestamp"].dt.hour*60 + x["timestamp"].dt.minute)
             .sort_values("timestamp")
             .groupby("date", as_index=False).first())
    days = [(now_kst().date() - timedelta(days=i)) for i in range(lookback_days)]
    existing = set(daily["date"].tolist())
    missing = [d for d in days if d not in existing]
    if len(daily) >= 5:
        mins = daily["minutes"].to_numpy()
        mu = float(np.mean(mins))
        sd = float(np.std(mins)) if np.std(mins) > 0 else 1.0
        zscores = (mins - mu) / sd
        out_idx = list(np.where(np.abs(zscores) > 2)[0])
        return {"missing_days": missing, "z_outliers_idx": out_idx, "mean_min": mu, "std_min": sd, "daily": daily}
    return {"missing_days": missing, "z_outliers_idx": [], "mean_min": None, "std_min": None, "daily": daily}

def enumerate_due_times(start_clock: dtime, interval_hours: int, from_dt: datetime, to_dt: datetime):
    start_at = datetime.combine(from_dt.date(), start_clock, tzinfo=KST)
    while start_at > from_dt:
        start_at -= timedelta(hours=interval_hours)
    while start_at + timedelta(hours=interval_hours) < from_dt:
        start_at += timedelta(hours=interval_hours)
    times, cur = [], start_at
    while cur <= to_dt:
        if cur >= from_dt: times.append(cur)
        cur += timedelta(hours=interval_hours)
    return times

def estimate_adherence(meds_df, med_log_df, days=7, window_minutes=60):
    to_dt = now_kst(); from_dt = to_dt - timedelta(days=days)
    due_list = []
    taken_list = med_log_df[(med_log_df["taken_at"]>=from_dt) & (med_log_df["taken_at"]<=to_dt)].copy()
    for _, row in meds_df.iterrows():
        name = row["name"]; iv = int(row["interval_hours"]); sc = parse_time_str(str(row["start_time"]))
        if not sc: continue
        for d in enumerate_due_times(sc, iv, from_dt, to_dt):
            due_list.append({"name": name, "due_time": d})
    due_df = pd.DataFrame(due_list)
    if due_df.empty: return 0, 0
    taken_on_time, window = 0, timedelta(minutes=window_minutes)
    for _, due in due_df.iterrows():
        name = due["name"]; dtime_ = due["due_time"]
        cand = taken_list[(taken_list["name"]==name) & (taken_list["taken_at"].between(dtime_-window, dtime_+window))]
        if len(cand):
            taken_on_time += 1
            taken_list = taken_list.drop(cand.index[0])
    return len(due_df), taken_on_time

def due_now_list(meds_df, within_minutes=15, overdue_minutes=90):
    now = now_kst(); due_items = []
    for _, row in meds_df.iterrows():
        name = row["name"]; iv = int(row["interval_hours"]); sc = parse_time_str(str(row["start_time"]))
        if not sc: continue
        dues = enumerate_due_times(sc, iv, now - timedelta(days=2), now + timedelta(days=1))
        if dues:
            closest = min(dues, key=lambda d: abs((d - now).total_seconds()))
            diff_min = (closest - now).total_seconds()/60.0
            status = "due" if abs(diff_min)<=within_minutes else ("overdue" if diff_min<0 and abs(diff_min)<=overdue_minutes else None)
            if status: due_items.append({"name": name, "due_time": closest, "status": status})
    return due_items

def risk_score(checkins_df, med_log_df, meds_df):
    cs = checkin_stats(checkins_df, lookback_days=14)
    missing_last3 = [d for d in cs.get("missing_days", []) if (now_kst().date() - d).days <= 3]
    n_missing3 = len(missing_last3); n_out7 = 0
    if "daily" in cs and len(cs["daily"])>0 and cs.get("mean_min") is not None and cs.get("std_min",0)>0:
        last7 = cs["daily"][cs["daily"]["date"] >= (now_kst().date()-timedelta(days=7))]
        if len(last7) >= 5:
            mins = last7["minutes"].to_numpy()
            z = (mins - cs["mean_min"]) / cs["std_min"]
            n_out7 = int(np.sum(np.abs(z)>2))
    adherence = 1.0
    if not meds_df.empty:
        due_total, taken_on_time = estimate_adherence(meds_df, med_log_df, days=7, window_minutes=60)
        adherence = (taken_on_time / due_total) if due_total>0 else 1.0
    score = min(n_missing3, 3)/3*40 + min(n_out7, 5)/5*20 + (1.0 - adherence)*40
    return round(max(0, min(100, score)), 1), {
        "missing_last3": n_missing3, "outliers_last7": n_out7, "adherence_7d": round(adherence*100,1)
    }

def haversine_km(lat1, lon1, lat2, lon2):
    R = 6371.0
    phi1, phi2 = np.radians(lat1), np.radians(lat2)
    dphi = np.radians(lat2-lat1); dlambda = np.radians(lon2-lon1)
    a = np.sin(dphi/2.0)**2 + np.cos(phi1)*np.cos(phi2)*np.sin(dlambda/2.0)**2
    return 2*R*np.arcsin(np.sqrt(a))

# =============================
# ê¸°ë³¸ ë°ì´í„° ë¡œë“œ + 5ë²ˆ ìë£Œ ìë™ ë°˜ì˜
# =============================
checkins = load_csv(CHECKIN_CSV, parse_dates=["timestamp"])
if checkins.empty:
    checkins = pd.DataFrame(columns=["timestamp"]); save_csv(checkins, CHECKIN_CSV)

meds = load_csv(MEDS_CSV)
if meds.empty:
    meds = pd.DataFrame(columns=["name","interval_hours","start_time","notes"]); save_csv(meds, MEDS_CSV)

med_log = load_csv(MEDLOG_CSV, parse_dates=["taken_at"])
if med_log.empty:
    med_log = pd.DataFrame(columns=["name","due_time","taken_at"]); save_csv(med_log, MEDLOG_CSV)

_user_inst = load_user_institutions()
if len(_user_inst): save_csv(_user_inst, INSTITUTIONS_CSV)
_user_reg = load_user_regional()
if len(_user_reg): save_csv(_user_reg, REGIONAL_CSV)

institutions = load_csv(INSTITUTIONS_CSV)
regional = load_csv(REGIONAL_CSV)

# =============================
# íƒ­
# =============================
tab1, tab2, tab3, tab4, tab5 = st.tabs(["â‘  ì²´í¬ì¸", "â‘¡ ìœ„í—˜ë„/119 ì‹œë‚˜ë¦¬ì˜¤", "â‘¢ ë³µì•½ ìŠ¤ì¼€ì¤„ëŸ¬", "â‘£ ì£¼ë³€ ì˜ë£Œê¸°ê´€ ì°¾ê¸°", "â‘¤ ë°ì´í„°/ì„¤ì •"])

# â‘  ì²´í¬ì¸
with tab1:
    st.header("â‘  ë§¤ì¼ ì²´í¬ì¸ (ì¼ì–´ë‚¨ ë²„íŠ¼)")
    c1, c2 = st.columns([1,2])
    with c1:
        if st.button("ğŸŒ ì¼ì–´ë‚¨(ì²´í¬ì¸)", use_container_width=True):
            checkins = pd.concat([checkins, pd.DataFrame([{"timestamp": now_kst()}])], ignore_index=True)
            save_csv(checkins, CHECKIN_CSV)
            st.success(f"ì²´í¬ì¸ ì™„ë£Œ: {now_kst().strftime('%Y-%m-%d %H:%M:%S')}")
    with c2:
        st.info("ì²´í¬ì¸ì€ í•˜ë£¨ 1íšŒ ì´ìƒ ê¶Œì¥í•©ë‹ˆë‹¤. ì•„ë˜ í‘œì—ì„œ ê¸°ë¡ì„ í™•ì¸í•˜ì„¸ìš”.")

    if not checkins.empty:
        st.subheader("ìµœê·¼ ì²´í¬ì¸ ê¸°ë¡")
        st.dataframe(checkins.sort_values("timestamp", ascending=False).head(50), use_container_width=True)
        df_plot = (checkins.assign(date=lambda x: x["timestamp"].dt.date,
                                   minutes=lambda x: x["timestamp"].dt.hour*60 + x["timestamp"].dt.minute)
                           .groupby("date", as_index=False)["minutes"].min()
                           .sort_values("date"))
        st.caption("ë‚ ì§œë³„ ì²« ì²´í¬ì¸ ì‹œê°(ë¶„)")
        st.line_chart(df_plot.set_index("date")["minutes"])
        cs = checkin_stats(checkins, lookback_days=30)
        st.markdown("**ìµœê·¼ 30ì¼ ê²°ì¸¡ì¼**")
        if cs.get("missing_days"):
            st.warning(", ".join(sorted([d.strftime("%Y-%m-%d") for d in cs["missing_days"]])))
        else:
            st.success("ê²°ì¸¡ ì—†ìŒ")
        st.markdown("**ì´ìƒì¹˜(|z|>2) ì˜ì‹¬(ì¼ë³„ ì²« ì²´í¬ì¸ ê¸°ì¤€)**")
        daily = cs.get("daily", pd.DataFrame()); out_idx = cs.get("z_outliers_idx", [])
        if len(out_idx) and len(daily)>0:
            st.error(daily.iloc[out_idx])
        else:
            st.success("ì´ìƒì¹˜ ì—†ìŒ")

# â‘¡ ìœ„í—˜ë„ / 119
with tab2:
    st.header("â‘¡ ìœ„í—˜ë„ ì˜ˆì¸¡ ë° ìë™ ì•Œë¦¼(ì‹œë®¬ë ˆì´ì…˜)")
    thr, info = st.columns([1,3])
    with thr:
        risk_thr = st.slider("119/ë³´í˜¸ì ì—°ë½(ê°€ìƒ) ë°œë™ ê¸°ì¤€(%)", 10, 100, 60, 5)
    with info:
        st.info("ì‹¤ì œ ì „í™” ë°œì‹ ì€ í•˜ì§€ ì•Šìœ¼ë©°, ì„ê³„ì¹˜ ì´ˆê³¼ ì‹œ ê²½ë³´ìŒê³¼ ì‹œë‚˜ë¦¬ì˜¤ ì•ˆë‚´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.")

    score, detail = risk_score(checkins, med_log, meds)
    st.subheader(f"í˜„ì¬ ìœ„í—˜ë„: {score}%")
    st.progress(min(1.0, score/100.0))
    c1, c2, c3 = st.columns(3)
    c1.metric("ìµœê·¼ 3ì¼ ê²°ì¸¡(ì¼)", detail["missing_last3"])
    c2.metric("ìµœê·¼ 7ì¼ ì´ìƒì¹˜(ì¼)", detail["outliers_last7"])
    c3.metric("ë³µì•½ ì¤€ìˆ˜(7ì¼)", f"{detail['adherence_7d']}%")

    if score >= risk_thr:
        st.error("âš ï¸ ìœ„í—˜ë„ ì„ê³„ì¹˜ ì´ˆê³¼! (ê°€ìƒ ê²½ë³´/ì—°ë½ ì‹œë‚˜ë¦¬ì˜¤)")
        st.audio(ALARM_WAV)
        st.markdown("""
**ì‹œë®¬ë ˆì´ì…˜: ìë™ ì—°ë½ ì ˆì°¨**
1) ë³´í˜¸ì 1ì°¨ ì—°ë½ ì‹œë„  
2) ë¯¸ì‘ë‹µ ì‹œ 119 ì—°ê³„ ì•ˆë‚´ ìŒì„± ì†¡ì¶œ  
3) ìœ„ì¹˜/ìµœê·¼ ì²´í¬ì¸/ë³µì•½ì •ë³´ ìš”ì•½ ì „ì†¡(ê°€ìƒ)
""")
    else:
        st.success("í˜„ì¬ëŠ” ì„ê³„ì¹˜ ë¯¸ë§Œì…ë‹ˆë‹¤.")

# â‘¢ ë³µì•½
with tab3:
    st.header("â‘¢ ë³µì•½ ìŠ¤ì¼€ì¤„ëŸ¬ / ë¦¬ë§ˆì¸ë”")
    st.caption("ì•±ì´ ì—´ë ¤ ìˆì„ ë•Œì—ë§Œ ë¦¬ë§ˆì¸ë”ê°€ í™”ë©´ì— í‘œì‹œë©ë‹ˆë‹¤(í”„ë¡œí† íƒ€ì… í•œê³„).")

    with st.form("add_med", clear_on_submit=True):
        st.subheader("ì•½ ì¶”ê°€")
        cx, cy, cz = st.columns([2,1,2])
        name = cx.text_input("ì•½ ì´ë¦„", placeholder="ì˜ˆ: ê³ í˜ˆì••ì•½A")
        interval = cy.number_input("ë³µìš© ê°„ê²©(ì‹œê°„)", 4, 48, 12, 1)
        start_t = cz.text_input("ì²« ë³µìš© ì‹œê°(HH:MM)", "08:00")
        notes = st.text_input("ë©”ëª¨(ì„ íƒ)", "")
        submit = st.form_submit_button("ì¶”ê°€")
        if submit and name and parse_time_str(start_t):
            meds = pd.concat([meds, pd.DataFrame([{
                "name": name, "interval_hours": int(interval), "start_time": start_t, "notes": notes
            }])], ignore_index=True)
            save_csv(meds, MEDS_CSV)
            st.success(f"ì¶”ê°€ë¨: {name} / {interval}ì‹œê°„ ê°„ê²© / ì‹œì‘ {start_t}")
        elif submit:
            st.error("ì…ë ¥ì„ í™•ì¸í•˜ì„¸ìš”. (ì‹œê° í˜•ì‹ HH:MM)")

    if len(meds):
        st.subheader("ë“±ë¡ëœ ì•½")
        st.dataframe(meds, use_container_width=True)
    else:
        st.info("ë“±ë¡ëœ ì•½ì´ ì—†ìŠµë‹ˆë‹¤.")

    if len(meds):
        st.subheader("ë¦¬ë§ˆì¸ë”")
        due_items = due_now_list(meds, within_minutes=15, overdue_minutes=90)
        if due_items:
            for item in due_items:
                name = item["name"]; due = item["due_time"].strftime("%Y-%m-%d %H:%M")
                status = "ğŸ•’ ê³§ ë³µì•½" if item["status"]=="due" else "â° ì—°ì²´"
                st.warning(f"{status}: {name} / ì˜ˆì •ì‹œê° {due}")
                b1, b2, _ = st.columns([1,1,3])
                with b1:
                    if st.button(f"âœ… {name} ë³µìš© ê¸°ë¡", key=f"take_{name}_{due}"):
                        med_log = pd.concat([med_log, pd.DataFrame([{
                            "name": name, "due_time": item["due_time"], "taken_at": now_kst()
                        }])], ignore_index=True)
                        save_csv(med_log, MEDLOG_CSV)
                        st.success(f"{name} ë³µìš© ê¸°ë¡ ì™„ë£Œ")
                with b2:
                    st.audio(ALARM_WAV)
        else:
            st.success("í˜„ì¬ 15ë¶„ ì´ë‚´ ì˜ˆì •/ì—°ì²´ í•­ëª© ì—†ìŒ")

    if len(meds):
        total7, ok7 = estimate_adherence(meds, med_log, days=7, window_minutes=60)
        if total7>0:
            st.metric("ìµœê·¼ 7ì¼ ì¤€ìˆ˜ìœ¨", f"{round(ok7/total7*100,1)}% ({ok7}/{total7})")
        else:
            st.info("ìµœê·¼ 7ì¼ ì˜ˆì • ìŠ¤ì¼€ì¤„ ì—†ìŒ")

    if len(med_log):
        st.subheader("ë³µìš© ê¸°ë¡")
        st.dataframe(med_log.sort_values("taken_at", ascending=False).head(100), use_container_width=True)

# â‘£ ì£¼ë³€ ì˜ë£Œê¸°ê´€(ì§‘ ìœ„ì¹˜ ì €ì¥/ì‚¬ìš©)
with tab4:
    st.header("â‘£ ì£¼ë³€ ì•½êµ­/ë³‘ì› ì°¾ê¸° ë° ì¶”ì²œ")
    st.caption("â€» 5ë²ˆ íƒ­ ìë£Œ(ì „êµ­ì˜ë£Œê¸°ê´€ í‘œì¤€ë°ì´í„°, ë…ê±°ë…¸ì¸ê°€êµ¬ ë¹„ìœ¨)ë¥¼ ìë™ ë°˜ì˜. í•„ìš” ì‹œ ì•„ë˜ì—ì„œ êµì²´ ì—…ë¡œë“œ ê°€ëŠ¥.")

    up1, up2 = st.columns(2)
    with up1:
        inst_file = st.file_uploader("ì „êµ­ ì˜ë£Œê¸°ê´€ í‘œì¤€ë°ì´í„° CSV ì—…ë¡œë“œ", type=["csv"])
        if inst_file is not None:
            institutions = normalize_institutions(pd.read_csv(inst_file))
            save_csv(institutions, INSTITUTIONS_CSV)
            st.success(f"ì—…ë¡œë“œ ì™„ë£Œ: {len(institutions)}ê°œ ê¸°ê´€")
    with up2:
        reg_file = st.file_uploader("ë…ê±°ë…¸ì¸ê°€êµ¬ ë¹„ìœ¨ íŒŒì¼ ì—…ë¡œë“œ (xlsx/csv)", type=["xlsx","csv"])
        if reg_file is not None:
            regional = normalize_regional(reg_file)
            save_csv(regional, REGIONAL_CSV)
            st.success(f"ì—…ë¡œë“œ ì™„ë£Œ: ì‹œë„ ë‹¨ìœ„ {len(regional)}ê°œ")

    if institutions.empty:
        st.info("ì˜ë£Œê¸°ê´€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. 5ë²ˆ ìë£Œê°€ ê°ì§€ë˜ì§€ ì•Šìœ¼ë©´ ì—¬ê¸°ì„œ ì—…ë¡œë“œí•˜ì„¸ìš”.")
    else:
        left, right = st.columns([2,1])
        with left:
            tsel = st.selectbox("ê¸°ê´€ ìœ í˜•", ["ì•½êµ­","ë³‘ì›","ì „ì²´"], index=0)
        with right:
            radius_km = st.slider("ê²€ìƒ‰ ë°˜ê²½(km)", 1, 20, 3)

        # === ì§‘ ìœ„ì¹˜ ===
        st.subheader("ë‚´ ìœ„ì¹˜(ìœ„ë„/ê²½ë„)")
        home = load_home()
        use_home = st.checkbox("ì €ì¥ëœ ì§‘ ìœ„ì¹˜ ì‚¬ìš©", value=(home is not None))

        if use_home and home is not None:
            st.success(f"ì§‘ ìœ„ì¹˜: {home['label']} (lat: {home['lat']:.6f}, lon: {home['lon']:.6f})")
            lat = float(home["lat"]); lon = float(home["lon"])
            cA, cB, cC = st.columns([1,1,2])
            with cA:
                if st.button("ì§‘ ìœ„ì¹˜ë¡œ ê²€ìƒ‰", use_container_width=True):
                    st.toast("ì§‘ ìœ„ì¹˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê²€ìƒ‰í•©ë‹ˆë‹¤.", icon="ğŸ ")
            with cB:
                if st.button("ì§‘ ìœ„ì¹˜ ì‚­ì œ", use_container_width=True):
                    delete_home()
                    st.experimental_rerun()
            with cC:
                st.caption("ì§‘ ìœ„ì¹˜ëŠ” ë¡œì»¬ íŒŒì¼(home_location.json)ì— ì €ì¥ë©ë‹ˆë‹¤.")
        else:
            lat = st.number_input("ìœ„ë„(lat)", value=37.5665, format="%.6f")
            lon = st.number_input("ê²½ë„(lon)", value=126.9780, format="%.6f")
            with st.expander("â• ì´ ìœ„ì¹˜ë¥¼ 'ì§‘'ìœ¼ë¡œ ì €ì¥"):
                home_label = st.text_input("í‘œì‹œ ì´ë¦„", value="ìš°ë¦¬ ì§‘")
                if st.button("ì´ ìœ„ì¹˜ë¥¼ ì§‘ìœ¼ë¡œ ì €ì¥", use_container_width=True):
                    save_home(lat, lon, home_label)
                    st.success(f"ì €ì¥ ì™„ë£Œ: {home_label} (lat: {lat:.6f}, lon: {lon:.6f})")
                    st.experimental_rerun()

        # í•„í„°/ê±°ë¦¬ê³„ì‚°
        df = institutions.copy()
        if tsel != "ì „ì²´": df = df[df["type"]==tsel]

        if {"lat","lon"}.issubset(df.columns) and len(df):
            df["distance_km"] = haversine_km(lat, lon, df["lat"].astype(float), df["lon"].astype(float))
            df = df[df["distance_km"]<=radius_km].sort_values("distance_km").reset_index(drop=True)

            # ì§€ì—­ ì·¨ì•½ë„ ê²°í•©(ì‹œë„ëª… ê¸°ì¤€)
            if not regional.empty and "region_name" in df.columns:
                r = regional.copy()
                if "solo_ratio" in r.columns:
                    r["solo_ratio_norm"] = r["solo_ratio"].astype(float).clip(0,1)
                if "accessibility_score" in r.columns and r["accessibility_score"].notna().any():
                    vals = r["accessibility_score"].astype(float)
                    r["accessibility_score_norm"] = 1.0 - (vals - vals.min())/(vals.max()-vals.min()+1e-9)
                r["regional_need"] = 0.0
                if "solo_ratio_norm" in r.columns: r["regional_need"] += 0.6*r["solo_ratio_norm"]
                if "accessibility_score_norm" in r.columns: r["regional_need"] += 0.4*r["accessibility_score_norm"]
                rr = r[["region_name","regional_need"]].drop_duplicates()
                df = df.merge(rr, on="region_name", how="left")
            else:
                df["regional_need"] = np.nan

            # ìµœì¢… ì¶”ì²œ ì ìˆ˜
            if len(df):
                df["proximity"] = 1.0 - (df["distance_km"] / (radius_km+1e-9))
                df["proximity"] = df["proximity"].clip(0,1)
                if df["regional_need"].notna().any():
                    df["rec_score"] = 0.6*df["proximity"] + 0.4*df["regional_need"].fillna(df["regional_need"].median())
                else:
                    df["rec_score"] = df["proximity"]

                # ì§€ë„(ì§‘ ë§ˆì»¤ + ê¸°ê´€ ë ˆì´ì–´)
                layers = []
                # ì§‘ ë§ˆì»¤
                layers.append(pdk.Layer(
                    "ScatterplotLayer",
                    data=pd.DataFrame([{"name":"ì§‘","lat":lat,"lon":lon}]),
                    get_position='[lon, lat]',
                    get_radius=80,
                    pickable=True,
                    get_fill_color=[255, 0, 0, 200],
                ))
                # ê¸°ê´€ ë§ˆì»¤
                layers.append(pdk.Layer(
                    "ScatterplotLayer",
                    data=df,
                    get_position='[lon, lat]',
                    get_radius=50,
                    pickable=True,
                    get_fill_color=[0, 128, 255, 160],
                ))
                view_state = pdk.ViewState(latitude=lat, longitude=lon, zoom=13)
                tooltip = {"text": "{name}\nê±°ë¦¬: {distance_km}km\nì¶”ì²œì ìˆ˜: {rec_score}"}
                st.subheader("ì§€ë„")
                st.pydeck_chart(pdk.Deck(layers=layers, initial_view_state=view_state, tooltip=tooltip))

                st.subheader("ê°€ê¹Œìš´ ìˆœ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸")
                show_cols = [c for c in ["name","type","address","region_name","distance_km","rec_score"] if c in df.columns]
                st.dataframe(df[show_cols].head(50), use_container_width=True)
            else:
                st.info("ë°˜ê²½ ë‚´ ê²°ê³¼ ì—†ìŒ.")
        else:
            st.error("ì˜ë£Œê¸°ê´€ ë°ì´í„°ì— lat/lon(ìœ„ë„/ê²½ë„) ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")

# â‘¤ ë°ì´í„°/ì„¤ì • (ìë£Œ ê´€ë¦¬)
with tab5:
    st.header("â‘¤ ë°ì´í„°/ì„¤ì • (ìë£Œ ê´€ë¦¬)")
    st.markdown("5ë²ˆ íƒ­ ìë£Œ(ì „êµ­ì˜ë£Œê¸°ê´€ í‘œì¤€ë°ì´í„°, ë…ê±°ë…¸ì¸ê°€êµ¬ ë¹„ìœ¨)ë¥¼ ìë™ ì¸ì‹í•˜ì—¬ í‘œì¤€ í¬ë§·ìœ¼ë¡œ ë³€í™˜Â·ìºì‹œí•©ë‹ˆë‹¤.")
    st.markdown("- **ì˜ë£Œê¸°ê´€ í‘œì¤€ CSV**: `institutions.csv`  \n- **ì§€ì—­ìš”ì¸ í‘œì¤€ CSV**: `regional_factors.csv`")

    c1, c2, c3, c4 = st.columns(4)
    with c1:
        st.download_button("ì²´í¬ì¸ CSV", data=checkins.to_csv(index=False).encode("utf-8"), file_name="checkins.csv")
    with c2:
        st.download_button("ì•½ ëª©ë¡ CSV", data=meds.to_csv(index=False).encode("utf-8"), file_name="meds.csv")
    with c3:
        st.download_button("ë³µì•½ ê¸°ë¡ CSV", data=med_log.to_csv(index=False).encode("utf-8"), file_name="med_log.csv")
    with c4:
        if not institutions.empty:
            st.download_button("ì˜ë£Œê¸°ê´€ CSV", data=institutions.to_csv(index=False).encode("utf-8"), file_name="institutions.csv")
        else:
            st.write("ì˜ë£Œê¸°ê´€ CSV: (ì—†ìŒ)")

    st.markdown("#### ìë™ ë¡œë“œ ìƒíƒœ ë¯¸ë¦¬ë³´ê¸°")
    ic, rc = st.columns(2)
    with ic:
        if len(_user_inst):
            st.success(f"ì „êµ­ì˜ë£Œê¸°ê´€ ì›ë³¸ ê°ì§€ë¨ âœ…  (í–‰ {len(_user_inst)}) â†’ í‘œì¤€ ë³€í™˜ ì €ì¥ ì™„ë£Œ")
        else:
            st.info("ì „êµ­ì˜ë£Œê¸°ê´€ ì›ë³¸ ë¯¸ê°ì§€. íƒ­4ì—ì„œ ì—…ë¡œë“œ ê°€ëŠ¥.")
        if not institutions.empty:
            st.dataframe(institutions.head(10), use_container_width=True)
    with rc:
        if len(_user_reg):
            st.success(f"ë…ê±°ë…¸ì¸ê°€êµ¬ ë¹„ìœ¨ ì›ë³¸ ê°ì§€ë¨ âœ…  (í–‰ {len(_user_reg)}) â†’ í‘œì¤€ ë³€í™˜ ì €ì¥ ì™„ë£Œ")
        else:
            st.info("ë…ê±°ë…¸ì¸ê°€êµ¬ ë¹„ìœ¨ ì›ë³¸ ë¯¸ê°ì§€. íƒ­4ì—ì„œ ì—…ë¡œë“œ ê°€ëŠ¥.")
        if not regional.empty:
            st.dataframe(regional.head(10), use_container_width=True)

    st.markdown("#### ìœ„í—˜ë„ ê³„ì‚°ì‹(ìš”ì•½)")
    st.code("""
# score = 0
# score += min(n_missing3, 3) / 3 * 40      # ìµœê·¼ 3ì¼ ê²°ì¸¡
# score += min(n_out7, 5) / 5 * 20          # ìµœê·¼ 7ì¼ ì´ìƒì¹˜(ì²´í¬ì¸ ì‹œê°)
# score += (1.0 - adherence) * 40           # 7ì¼ ë³µì•½ ì¤€ìˆ˜ìœ¨ ì—­ê°€ì¤‘
# => 0~100 ì ìˆ˜
""", language="python")

# =============================
# ìƒíƒœ ì €ì¥
# =============================
save_csv(checkins, CHECKIN_CSV)
save_csv(meds, MEDS_CSV)
save_csv(med_log, MEDLOG_CSV)
if len(institutions): save_csv(institutions, INSTITUTIONS_CSV)
if len(regional): save_csv(regional, REGIONAL_CSV)
